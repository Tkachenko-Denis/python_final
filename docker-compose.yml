version: '3.9'

services:
  postgres:
    image: postgres:latest
    container_name: postgres_container
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: source_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_db/init_postgres.sql:/docker-entrypoint-initdb.d/init_postgres.sql

  mysql:
    image: mysql:latest
    container_name: mysql_container
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: target_db
      MYSQL_USER: user
      MYSQL_PASSWORD: password
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init_db/init_mysql.sql:/docker-entrypoint-initdb.d/init_mysql.sql

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  data_generator:
    build:
      context: .
      dockerfile: ./generate_data/Dockerfile
    container_name: data_generator
    depends_on:
      - postgres
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: source_db
      DB_USER: admin
      DB_PASSWORD: admin
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      python /app/generate_data.py"

  data_generator_kafka:
    build:
      context: .
      dockerfile: ./generate_data_for_kafka/Dockerfile
    container_name: data_generator_kafka
    depends_on:
      - kafka
    environment:
      KAFKA_HOST: kafka
      KAFKA_PORT: 9092
      TOPIC_NAME: orders
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      python /app/generate_data_for_kafka.py"

  spark:
    image: bitnami/spark:latest
    container_name: spark
    depends_on:
      - kafka
    environment:
      SPARK_MASTER_URL: spark://spark:7077
    ports:
      - "8081:8081" # Spark Web UI
    command: spark-class org.apache.spark.deploy.master.Master

  spark_processor:
    build:
      context: .
      dockerfile: ./spark_processor/Dockerfile
    container_name: spark_processor
    depends_on:
      - kafka
      - postgres
    environment:
      SPARK_MASTER_URL: spark://spark:7077
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: source_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: orders
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      spark-submit --master local[*] /app/spark_processor.py"

  airflow:
    image: apache/airflow:2.7.1
    container_name: airflow_container
    environment:
      AIRFLOW_UID: 50000
    depends_on:
      - postgres
      - mysql
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/opt/airflow/requirements.txt
    entrypoint: >
      /bin/bash -c "
      pip install -r /opt/airflow/requirements.txt &&
      airflow db migrate &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
      airflow standalone"

volumes:
  postgres_data:
  mysql_data:

